# ML is All You Need

This repo provides few-sentence summaries of trending ML papers mostly focused on time series forecasting, foundation models and anomaly detection. We skip over dataset descriptions (spoiler: everyone uses ETT) and result sections (surprise: everyone's state-of-the-art). Instead, the focus is on the core technical details auch as architecture, preprocessing, learning objective, and what makes the method unique.

## ðŸ“š Paper Index

| Title | Paper | Code | Publisher | Year | Topics |
|-------|-------|------|-----------|------|--------|
| [Energy-Based Transformers are Scalable Learners and Thinkers](papers/energy-based-transformers-are-scalable-learners-and-thinkers.md) | [arXiv](https://arxiv.org/pdf/2507.02092) | [GitHub](https://github.com/alexiglad/ebt) |   | 2025 | `Energy`
| [Learning without training: The implicit dynamics of in-context learning](papers/learning-without-training-the-implicit-dynamics-of-in-context-learning.md) | [arXiv](https://arxiv.org/pdf/2507.16003) |   | Google | 2025 | `Context`, `LLM`
| [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](papers/tirex-zero-shot-forecasting-across-long-and-short-horizons-with-enhanced-in-context-learning.md) | [arXiv](https://arxiv.org/pdf/2505.23719) | [GitHub](https://github.com/NX-AI/tirex) |   | 2025 | `FM`, `Patching`, `Probabilistic`
| [Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts](papers/time-moe-billion-scale-time-series-foundation-models-with-mixture-of-experts.md) | [arXiv](https://arxiv.org/pdf/2409.16040) | [GitHub](https://github.com/Time-MoE/Time-MoE) |   | 2025 | `FM`, `MoE`, `Multi-Horizon`
| [Aurora: A foundation model for the Earth system](papers/aurora-a-foundation-model-for-the-earth-system.md) | [Nature](https://www.nature.com/articles/s41586-025-09005-y) | [GitHub](https://github.com/microsoft/aurora) | Microsoft | 2025 | `FM`, `Patching`
| [ChronosX: Adapting Pretrained Time Series Models with Exogenous Variables](papers/chronosx-adapting-pretrained-time-series-models-with-exogenous-variables.md) | [arXiv](https://arxiv.org/pdf/2503.12107) | [GitHub](https://github.com/amazon-science/chronos-forecasting/tree/chronosx) | Amazon | 2025 | `Covariates`, `FM`, `LLM`
| [MOIRAI: Unified Training of Universal Time Series Forecasting Transformers](papers/moirai-unified-training-of-universal-time-series-forecasting-transformers.md) | [arXiv](https://arxiv.org/pdf/2402.02592) | [GitHub](https://github.com/SalesforceAIResearch/uni2ts) | Salesforce | 2024 | `Covariates`, `FM`, `Patching`, `Probabilistic`
| [Are Language Models Actually Useful for Time Series Forecasting?](papers/are-language-models-actually-useful-for-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/2406.16964) | [GitHub](https://github.com/BennyTMT/LLMsForTimeSeries) |   | 2024 | `LLM`
| [xLSTM: Extended Long Short-Term Memory](papers/xlstm-extended-long-short-term-memory.md) | [arXiv](https://arxiv.org/pdf/2405.04517) | [GitHub](https://github.com/NX-AI/xlstm) | NXAI | 2024 | `Memory`
| [MOMENT: A Family of Open Time-series Foundation Models](papers/moment-a-family-of-open-time-series-foundation-models.md) | [arXiv](https://arxiv.org/pdf/2402.03885) | [GitHub](https://github.com/moment-timeseries-foundation-model/moment) |   | 2024 | `Anomaly`, `FM`, `Patching`, `Representation`
| [Chronos: Learning the Language of Time Series](papers/chronos-learning-the-language-of-time-series.md) | [arXiv](https://arxiv.org/pdf/2403.07815) | [GitHub](https://github.com/amazon-science/chronos-forecasting) | Amazon | 2024 | `FM`, `LLM`, `Probabilistic`
| [Titans: Learning to Memorize at Test Time](papers/titans-learning-to-memorize-at-test-time.md) | [arXiv](https://arxiv.org/pdf/2501.00663) |   | Google | 2024 | `FM`, `Memory`, `Static`
| [Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting](papers/lag-llama-towards-foundation-models-for-probabilistic-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/2310.08278) | [GitHub](https://github.com/time-series-foundation-models/lag-llama) |   | 2024 | `Covariates`, `FM`, `Multi-Horizon`, `Probabilistic`
| [TimesFM: A decoder-only foundation model for time-series forecasting](papers/timesfm-a-decoder-only-foundation-model-for-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/2310.10688) |   | Google | 2024 | `FM`, `Multi-Horizon`, `Patching`
| [JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](papers/jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture.md) | [arXiv](https://arxiv.org/pdf/2301.08243) |   | Meta | 2023 | `CV`, `Representation`
| [PatchTST: A Time Series is Worth 64 Words - Long-term Forecasting with Transformers](papers/patchtst-a-time-series-is-worth-64-words-long-term-forecasting-with-transformers.md) | [arXiv](https://arxiv.org/pdf/2211.14730) | [GitHub](https://github.com/yuqinie98/PatchTST) | IBM | 2023 | `Patching`, `Representation`
| [N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting](papers/n-hits-neural-hierarchical-interpolation-for-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/2201.12886) | [GitHub](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/nhits.py) | Nixtla | 2022 | `Interpretable`, `Multi-Horizon`
| [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](papers/informer-beyond-efficient-transformer-for-long-sequence-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/2012.07436) | [GitHub](https://github.com/zhouhaoyi/Informer2020) |   | 2021 | `FM`, `Multi-Horizon`
| [GDN: Graph Neural Network-Based Anomaly Detection in Multivariate Time Series](papers/gdn-graph-neural-network-based-anomaly-detection-in-multivariate-time-series.md) | [arXiv](https://arxiv.org/pdf/2106.06947) | [GitHub](https://github.com/d-ailin/GDN) |   | 2021 | `Anomaly`
| [TFT: Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](papers/tft-temporal-fusion-transformers-for-interpretable-multi-horizon-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/1912.09363) |   | Google | 2020 | `Covariates`, `Probabilistic`, `Static`
| [N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting](papers/n-beats-neural-basis-expansion-analysis-for-interpretable-time-series-forecasting.md) | [arXiv](https://arxiv.org/pdf/1905.10437) | [GitHub](https://github.com/ServiceNow/N-BEATS) | Element AI | 2020 | `Interpretable`
| [USAD: UnSupervised Anomaly Detection on Multivariate Time Series](papers/usad-unsupervised-anomaly-detection-on-multivariate-time-series.md) | [ACM Digital Library](https://dl.acm.org/doi/pdf/10.1145/3394486.3403392) |   |   | 2020 | `Anomaly`
| [MTAD-GAT: Multivariate Time-series Anomaly Detection via Graph Attention Network](papers/mtad-gat-multivariate-time-series-anomaly-detection-via-graph-attention-network.md) | [arXiv](https://arxiv.org/pdf/2009.02040.pdf) | [GitHub](https://github.com/ML4ITS/mtad-gat-pytorch) |   | 2020 | `Anomaly`

